"""
Comprehensive Autonomous Test Suite - Full SDLC Validation.

Runs all test suites across all generations and provides comprehensive
validation of the entire autonomous SDLC implementation.
"""

import pytest
import time
import logging
import json
from pathlib import Path
import sys
from typing import Dict, Any, List
from datetime import datetime
import subprocess

# Add project root
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Import all test modules and systems
from olfactory_transformer.utils.dependency_isolation import dependency_manager, print_system_status
from olfactory_transformer.utils.advanced_error_handling import error_handler
from olfactory_transformer.utils.intelligent_monitoring import system_monitor
from olfactory_transformer.utils.quantum_performance_optimizer import quantum_optimizer
from olfactory_transformer.utils.intelligent_scaling import intelligent_scaler

logger = logging.getLogger(__name__)

class AutonomousTestOrchestrator:
    """Orchestrates comprehensive autonomous testing."""
    
    def __init__(self):
        self.test_results: Dict[str, Any] = {}
        self.start_time = datetime.now()
        self.system_metrics_before: Dict[str, Any] = {}
        self.system_metrics_after: Dict[str, Any] = {}
        
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """Run all comprehensive tests and return results."""
        
        print("ðŸš€ AUTONOMOUS SDLC - COMPREHENSIVE TEST EXECUTION")
        print("=" * 70)
        
        # Capture initial system state
        self._capture_initial_metrics()
        
        # Run all test generations
        test_suites = [
            ("Generation 1 - Basic Functionality", self._run_generation_1_tests),
            ("Generation 2 - Robustness", self._run_generation_2_tests), 
            ("Generation 3 - Optimization", self._run_generation_3_tests),
            ("Integration Tests", self._run_integration_tests),
            ("Performance Benchmarks", self._run_performance_tests),
            ("Security Validation", self._run_security_tests)
        ]
        
        for suite_name, test_func in test_suites:
            print(f"\nðŸ“‹ Executing {suite_name}...")
            try:
                result = test_func()
                self.test_results[suite_name] = result
                status = "âœ… PASSED" if result.get("success", False) else "âŒ FAILED"
                print(f"   {status} - {result.get('summary', 'No summary')}")
            except Exception as e:
                self.test_results[suite_name] = {
                    "success": False,
                    "error": str(e),
                    "summary": f"Test suite failed: {e}"
                }
                print(f"   âŒ FAILED - {e}")
                
        # Capture final system state
        self._capture_final_metrics()
        
        # Generate comprehensive report
        comprehensive_report = self._generate_comprehensive_report()
        
        return comprehensive_report
        
    def _capture_initial_metrics(self):
        """Capture initial system metrics."""
        self.system_metrics_before = {
            "dependency_health": dependency_manager.check_system_health(),
            "error_handler_status": error_handler.get_health_report(),
            "monitor_status": system_monitor.get_health_status(),
            "optimizer_report": quantum_optimizer.get_performance_report(),
            "scaler_status": intelligent_scaler.get_scaling_status(),
            "timestamp": datetime.now().isoformat()
        }
        
    def _capture_final_metrics(self):
        """Capture final system metrics."""
        self.system_metrics_after = {
            "dependency_health": dependency_manager.check_system_health(),
            "error_handler_status": error_handler.get_health_report(),
            "monitor_status": system_monitor.get_health_status(),
            "optimizer_report": quantum_optimizer.get_performance_report(),
            "scaler_status": intelligent_scaler.get_scaling_status(),
            "timestamp": datetime.now().isoformat()
        }
        
    def _run_generation_1_tests(self) -> Dict[str, Any]:
        """Run Generation 1 basic functionality tests."""
        
        try:
            # Run the basic autonomous test
            result = subprocess.run([
                sys.executable, "test_autonomous_basic.py", "-v"
            ], capture_output=True, text=True, timeout=60)
            
            success = result.returncode == 0
            test_output = result.stdout + result.stderr
            
            return {
                "success": success,
                "return_code": result.returncode,
                "summary": f"Basic tests {'passed' if success else 'failed'}",
                "details": test_output[:500],  # Limit output size
                "metrics": {
                    "dependency_coverage": len(dependency_manager._dependencies),
                    "import_success": True  # If we got this far, imports worked
                }
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "summary": "Generation 1 tests timed out",
                "error": "Test execution exceeded 60 seconds"
            }
        except Exception as e:
            return {
                "success": False,
                "summary": "Generation 1 tests failed to execute",
                "error": str(e)
            }
            
    def _run_generation_2_tests(self) -> Dict[str, Any]:
        """Run Generation 2 robustness tests."""
        
        try:
            result = subprocess.run([
                sys.executable, "test_generation_2_robustness.py", "-v"
            ], capture_output=True, text=True, timeout=120)
            
            success = result.returncode == 0
            test_output = result.stdout + result.stderr
            
            # Extract test statistics
            passed_count = test_output.count("PASSED")
            failed_count = test_output.count("FAILED")
            
            return {
                "success": success,
                "return_code": result.returncode,
                "summary": f"Robustness tests: {passed_count} passed, {failed_count} failed",
                "details": test_output[:500],
                "metrics": {
                    "tests_passed": passed_count,
                    "tests_failed": failed_count,
                    "error_handling_active": len(error_handler.error_history) > 0,
                    "monitoring_active": system_monitor.monitoring_active
                }
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "summary": "Generation 2 tests timed out",
                "error": "Test execution exceeded 120 seconds"
            }
        except Exception as e:
            return {
                "success": False,
                "summary": "Generation 2 tests failed to execute", 
                "error": str(e)
            }
            
    def _run_generation_3_tests(self) -> Dict[str, Any]:
        """Run Generation 3 optimization tests."""
        
        try:
            result = subprocess.run([
                sys.executable, "test_generation_3_optimization.py", "-v"
            ], capture_output=True, text=True, timeout=120)
            
            success = result.returncode == 0
            test_output = result.stdout + result.stderr
            
            # Extract performance metrics
            passed_count = test_output.count("PASSED")
            failed_count = test_output.count("FAILED")
            
            return {
                "success": success,
                "return_code": result.returncode,
                "summary": f"Optimization tests: {passed_count} passed, {failed_count} failed",
                "details": test_output[:500],
                "metrics": {
                    "tests_passed": passed_count,
                    "tests_failed": failed_count,
                    "quantum_cache_size": len(quantum_optimizer.cache.cache),
                    "scaling_active": len(intelligent_scaler.scaling_history) > 0
                }
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "summary": "Generation 3 tests timed out",
                "error": "Test execution exceeded 120 seconds"
            }
        except Exception as e:
            return {
                "success": False,
                "summary": "Generation 3 tests failed to execute",
                "error": str(e)
            }
            
    def _run_integration_tests(self) -> Dict[str, Any]:
        """Run integration tests across all systems."""
        
        integration_results = []
        
        # Test 1: Cross-system communication
        try:
            from olfactory_transformer import OlfactoryTransformer, MoleculeTokenizer
            
            # Test basic workflow
            tokenizer = MoleculeTokenizer()
            tokens = tokenizer.tokenize("CCO")  # Ethanol
            
            transformer = OlfactoryTransformer()
            # In mock mode, this should return a mock prediction
            
            integration_results.append({
                "test": "Basic workflow integration",
                "success": True,
                "details": f"Tokenization produced {len(tokens)} tokens"
            })
            
        except Exception as e:
            integration_results.append({
                "test": "Basic workflow integration", 
                "success": False,
                "error": str(e)
            })
            
        # Test 2: Error handling integration
        try:
            from olfactory_transformer.utils.advanced_error_handling import robust_operation, ErrorSeverity
            
            @robust_operation("integration_test", "test_operation", ErrorSeverity.LOW)
            def test_function():
                raise ValueError("Test integration error")
                
            result = test_function()  # Should return graceful degradation
            
            integration_results.append({
                "test": "Error handling integration",
                "success": result is not None,
                "details": f"Error gracefully handled, returned: {type(result)}"
            })
            
        except Exception as e:
            integration_results.append({
                "test": "Error handling integration",
                "success": False, 
                "error": str(e)
            })
            
        # Test 3: Monitoring integration
        try:
            from olfactory_transformer.utils.intelligent_monitoring import monitor_operation
            
            @monitor_operation("integration_test", "monitoring_component")
            def monitored_function(x):
                time.sleep(0.01)
                return x * 2
                
            result = monitored_function(5)
            
            integration_results.append({
                "test": "Monitoring integration",
                "success": result == 10,
                "details": f"Monitored function returned: {result}"
            })
            
        except Exception as e:
            integration_results.append({
                "test": "Monitoring integration",
                "success": False,
                "error": str(e)
            })
            
        success_count = sum(1 for r in integration_results if r["success"])
        total_tests = len(integration_results)
        
        return {
            "success": success_count == total_tests,
            "summary": f"Integration tests: {success_count}/{total_tests} passed",
            "details": integration_results,
            "metrics": {
                "tests_passed": success_count,
                "tests_total": total_tests,
                "success_rate": success_count / total_tests if total_tests > 0 else 0
            }
        }
        
    def _run_performance_tests(self) -> Dict[str, Any]:
        """Run performance benchmark tests."""
        
        performance_results = []
        
        # Test 1: Tokenization performance
        try:
            from olfactory_transformer.core.tokenizer import MoleculeTokenizer
            
            tokenizer = MoleculeTokenizer()
            test_smiles = ["CCO", "CC(C)CC1=CC=C(C=C1)C(C)C", "CCOC(=O)C1=CC=CC=C1"]
            
            start_time = time.time()
            for smiles in test_smiles * 10:  # 30 total operations
                tokens = tokenizer.tokenize(smiles)
            tokenization_time = time.time() - start_time
            
            performance_results.append({
                "test": "Tokenization performance",
                "success": tokenization_time < 1.0,  # Should complete in < 1 second
                "details": f"30 tokenizations in {tokenization_time:.3f}s",
                "metrics": {
                    "operations": 30,
                    "total_time": tokenization_time,
                    "ops_per_second": 30 / tokenization_time
                }
            })
            
        except Exception as e:
            performance_results.append({
                "test": "Tokenization performance",
                "success": False,
                "error": str(e)
            })
            
        # Test 2: Cache performance
        try:
            from olfactory_transformer.utils.quantum_performance_optimizer import quantum_optimize
            
            @quantum_optimize(cache_key="perf_test")
            def cached_computation(x):
                time.sleep(0.01)  # Simulate work
                return x ** 2
                
            # First call (cache miss)
            start_time = time.time()
            result1 = cached_computation(10)
            first_call_time = time.time() - start_time
            
            # Second call (should be faster due to caching)
            start_time = time.time()
            result2 = cached_computation(10)
            second_call_time = time.time() - start_time
            
            cache_effectiveness = first_call_time > second_call_time or second_call_time < 0.005
            
            performance_results.append({
                "test": "Cache performance",
                "success": cache_effectiveness and result1 == result2 == 100,
                "details": f"First: {first_call_time:.3f}s, Second: {second_call_time:.3f}s",
                "metrics": {
                    "first_call_time": first_call_time,
                    "second_call_time": second_call_time,
                    "speedup_factor": first_call_time / max(second_call_time, 0.001)
                }
            })
            
        except Exception as e:
            performance_results.append({
                "test": "Cache performance",
                "success": False,
                "error": str(e)
            })
            
        # Test 3: Scaling system responsiveness
        try:
            from olfactory_transformer.utils.intelligent_scaling import auto_scale_resource, ResourceType
            
            @auto_scale_resource(ResourceType.THREADS)
            def scaling_test_function(iterations):
                for _ in range(iterations):
                    time.sleep(0.001)
                return iterations
                
            start_time = time.time()
            results = []
            for i in range(10):
                result = scaling_test_function(i * 5)
                results.append(result)
            scaling_test_time = time.time() - start_time
            
            performance_results.append({
                "test": "Scaling system responsiveness", 
                "success": scaling_test_time < 2.0 and len(results) == 10,
                "details": f"10 scaled operations in {scaling_test_time:.3f}s",
                "metrics": {
                    "operations": len(results),
                    "total_time": scaling_test_time,
                    "ops_per_second": len(results) / scaling_test_time
                }
            })
            
        except Exception as e:
            performance_results.append({
                "test": "Scaling system responsiveness",
                "success": False,
                "error": str(e)
            })
            
        success_count = sum(1 for r in performance_results if r["success"])
        total_tests = len(performance_results)
        
        return {
            "success": success_count >= total_tests * 0.8,  # Allow 20% margin
            "summary": f"Performance tests: {success_count}/{total_tests} passed",
            "details": performance_results,
            "metrics": {
                "tests_passed": success_count,
                "tests_total": total_tests,
                "success_rate": success_count / total_tests if total_tests > 0 else 0
            }
        }
        
    def _run_security_tests(self) -> Dict[str, Any]:
        """Run security validation tests."""
        
        security_results = []
        
        # Test 1: Input validation
        try:
            from olfactory_transformer.core.tokenizer import MoleculeTokenizer
            
            tokenizer = MoleculeTokenizer()
            
            # Test with potentially malicious input
            malicious_inputs = [
                "",  # Empty string
                "A" * 10000,  # Very long string
                "../../../etc/passwd",  # Path traversal attempt
                "<script>alert('xss')</script>",  # XSS attempt
                "'; DROP TABLE molecules; --"  # SQL injection attempt
            ]
            
            safe_handling_count = 0
            for malicious_input in malicious_inputs:
                try:
                    result = tokenizer.tokenize(malicious_input)
                    # Should either handle gracefully or return safe result
                    safe_handling_count += 1
                except Exception:
                    # Exceptions are acceptable for malicious input
                    safe_handling_count += 1
                    
            security_results.append({
                "test": "Input validation security",
                "success": safe_handling_count == len(malicious_inputs),
                "details": f"Safely handled {safe_handling_count}/{len(malicious_inputs)} malicious inputs"
            })
            
        except Exception as e:
            security_results.append({
                "test": "Input validation security",
                "success": False,
                "error": str(e)
            })
            
        # Test 2: Error information leakage
        try:
            from olfactory_transformer.utils.advanced_error_handling import error_handler
            
            # Generate a test error
            try:
                raise ValueError("Sensitive internal information: password123")
            except ValueError as e:
                result = error_handler.handle_error(
                    e, "security_test", "test_operation"
                )
                
            # Check that error details are not leaked in result
            result_str = json.dumps(result) if result else ""
            leaked_info = "password123" in result_str.lower()
            
            security_results.append({
                "test": "Error information leakage",
                "success": not leaked_info,
                "details": f"Sensitive info {'leaked' if leaked_info else 'protected'}"
            })
            
        except Exception as e:
            security_results.append({
                "test": "Error information leakage",
                "success": False,
                "error": str(e)
            })
            
        # Test 3: Resource limits
        try:
            from olfactory_transformer.utils.intelligent_scaling import intelligent_scaler
            
            # Check that resource limits are enforced
            limits_enforced = True
            for resource_type, capacity in intelligent_scaler.current_capacity.items():
                limits = intelligent_scaler.resource_limits[resource_type]
                if not (limits["min"] <= capacity <= limits["max"]):
                    limits_enforced = False
                    break
                    
            security_results.append({
                "test": "Resource limits enforcement",
                "success": limits_enforced,
                "details": f"Resource limits {'enforced' if limits_enforced else 'violated'}"
            })
            
        except Exception as e:
            security_results.append({
                "test": "Resource limits enforcement",
                "success": False,
                "error": str(e)
            })
            
        success_count = sum(1 for r in security_results if r["success"])
        total_tests = len(security_results)
        
        return {
            "success": success_count == total_tests,  # Security must be 100%
            "summary": f"Security tests: {success_count}/{total_tests} passed",
            "details": security_results,
            "metrics": {
                "tests_passed": success_count,
                "tests_total": total_tests,
                "success_rate": success_count / total_tests if total_tests > 0 else 0
            }
        }
        
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report."""
        
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        # Calculate overall success
        suite_successes = [result.get("success", False) for result in self.test_results.values()]
        overall_success = all(suite_successes)
        
        # System health comparison
        health_change = self._calculate_health_change()
        
        # Compile all metrics
        comprehensive_metrics = {
            "total_test_suites": len(self.test_results),
            "successful_suites": sum(suite_successes),
            "test_duration_seconds": total_duration,
            "overall_success": overall_success,
            "health_change": health_change,
            "system_stability": self._assess_system_stability(),
            "performance_impact": self._calculate_performance_impact()
        }
        
        return {
            "timestamp": end_time.isoformat(),
            "overall_success": overall_success,
            "test_duration": total_duration,
            "suite_results": self.test_results,
            "system_metrics_before": self.system_metrics_before,
            "system_metrics_after": self.system_metrics_after,
            "comprehensive_metrics": comprehensive_metrics,
            "recommendations": self._generate_recommendations()
        }
        
    def _calculate_health_change(self) -> Dict[str, Any]:
        """Calculate system health changes during testing."""
        
        before = self.system_metrics_before
        after = self.system_metrics_after
        
        return {
            "dependency_health_change": (
                after["dependency_health"]["available"] - 
                before["dependency_health"]["available"]
            ),
            "error_rate_change": (
                after["error_handler_status"]["total_errors"] -
                before["error_handler_status"]["total_errors"]
            ),
            "monitoring_health_stable": (
                before["monitor_status"]["overall_health"] ==
                after["monitor_status"]["overall_health"]
            )
        }
        
    def _assess_system_stability(self) -> str:
        """Assess overall system stability."""
        
        # Check for critical issues
        after_metrics = self.system_metrics_after
        
        critical_issues = []
        if after_metrics["monitor_status"]["critical_alerts"] > 0:
            critical_issues.append("Critical monitoring alerts")
        if after_metrics["error_handler_status"]["system_health"] == "DEGRADED":
            critical_issues.append("Error handler reports system degradation")
        if after_metrics["dependency_health"]["critical_missing"]:
            critical_issues.append("Critical dependencies missing")
            
        if critical_issues:
            return f"UNSTABLE: {', '.join(critical_issues)}"
        elif after_metrics["monitor_status"]["overall_health"] == "healthy":
            return "STABLE"
        else:
            return "DEGRADED"
            
    def _calculate_performance_impact(self) -> Dict[str, float]:
        """Calculate performance impact of testing."""
        
        before_opt = self.system_metrics_before["optimizer_report"]
        after_opt = self.system_metrics_after["optimizer_report"]
        
        return {
            "cache_size_change": after_opt["cache_size"] - before_opt["cache_size"],
            "optimization_count_change": (
                after_opt["total_optimizations"] - before_opt["total_optimizations"]
            ),
            "cache_hit_rate_change": (
                after_opt["cache_hit_rate"] - before_opt["cache_hit_rate"]
            )
        }
        
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results."""
        
        recommendations = []
        
        # Check for failed test suites
        for suite_name, result in self.test_results.items():
            if not result.get("success", False):
                recommendations.append(f"Investigate failures in {suite_name}")
                
        # Check system health
        health_change = self._calculate_health_change()
        if health_change["error_rate_change"] > 10:
            recommendations.append("High error rate increase detected - review error handling")
            
        # Check performance
        perf_impact = self._calculate_performance_impact()
        if perf_impact["cache_hit_rate_change"] < -0.1:
            recommendations.append("Cache hit rate decreased - review caching strategy")
            
        if not recommendations:
            recommendations.append("All systems operating within normal parameters")
            
        return recommendations

def main():
    """Main execution function."""
    
    # Initialize logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Print system status
    print_system_status()
    
    # Run comprehensive tests
    orchestrator = AutonomousTestOrchestrator()
    comprehensive_report = orchestrator.run_comprehensive_tests()
    
    # Print final results
    print("\n" + "=" * 70)
    print("ðŸ† AUTONOMOUS SDLC - FINAL RESULTS")
    print("=" * 70)
    
    overall_success = comprehensive_report["overall_success"]
    status_emoji = "âœ…" if overall_success else "âŒ"
    print(f"{status_emoji} Overall Success: {overall_success}")
    print(f"â±ï¸  Total Duration: {comprehensive_report['test_duration']:.2f} seconds")
    print(f"ðŸ“Š Test Suites: {comprehensive_report['comprehensive_metrics']['successful_suites']}/{comprehensive_report['comprehensive_metrics']['total_test_suites']} passed")
    print(f"ðŸ¥ System Stability: {comprehensive_report['comprehensive_metrics']['system_stability']}")
    
    print(f"\nðŸ’¡ Recommendations:")
    for i, rec in enumerate(comprehensive_report['recommendations'], 1):
        print(f"   {i}. {rec}")
        
    # Save comprehensive report
    report_file = Path("comprehensive_test_report.json")
    with open(report_file, 'w') as f:
        json.dump(comprehensive_report, f, indent=2, default=str)
    print(f"\nðŸ“„ Detailed report saved to: {report_file}")
    
    return 0 if overall_success else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)